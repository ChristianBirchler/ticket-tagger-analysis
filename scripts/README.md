# Scripts
Here you will find all Python scripts created and used for this project. In this readme, you can read about their meaning, usage and restrictions. 
>Click on the headers to inspect the linked scripts / folders.

## [Classifiers](./classifiers)
This folder is home to two fasttext classifiers. Both are tested with a fasttext data set text file as input and generate a json containing the results of a 10-fold validation. The meaned results are in "results" and in "details" you can see the folds themselfes.

`classifier.py` is a stock classifier, the calculated Percision/Recall/F1 is generated by the `model.test()` function, which differs from the evaluation used in [ticket-tagger](https://github.com/rafaelkallis/ticket-tagger/tree/master/src). 

`ml_bin_classifier.py` on the other hand, uses a hand crafted multi-label-binary approach. It takes a **normal** fasttext data set as input and uses the [create_binary_datasets.py](./data_acquisition/create_binary_datasets.py) script to split it into 3 binary ones. During this process, temporary files are created and deleted. The structure of the output file is similar to the other classifier.
 >Both classifiers use a fasttext text file data set as input and produce a json file with the results of the 10-fold validation.

### [Stock Classifier](./classifiers/classifier.py)
    python classifier.py <INPUT_DATASET.txt> <OUTPUT.json>

### [Multi-Label-Binary Classifier](./classifiers/ml_bin_classifier.py)
    python ml_bin_classifier.py <INPUT_DATASET.txt> <OUTPUT.json>

## [Data Acquisition](./data_acquisition)

In this folder resides a collection of scripts to facilitate the process of data acquisition. 
The idea is to first use the `dump_issues` script to scrape GitHub for issues and get a json file.
In order to translate this json dump into a fasttext data format, use the `json2fasttext` script.
If you need to split this dataset into a binary dataset, translate any multi-label fasttext data with the help of the `create_binary_datasets` script.
If you want to use WEKA/MEKA, you will need to use the ARFF-Conversion scripts.  
#### **As of 12.10.2020 classified labels from pandas issues:**
- questions (33 - Question): 105
- enhancement (01 - Enhancement, 23 - Wish List): 1625
- bug (00 - Bug, 06 - Regression): 5170
- total: 6900

### [Github Scraper](./data_acquisition/dump_issues.py)

 >Make Sure to edit the variables `username` and `token` in the script to match your GitHub API credentials before using the script, as it will not work otherwise.
        
    python dump_issues.py <OWNER_NAME> <REPO_NAME>

### [Fasttext Converter](./data_acquisition/json2fasttext.py)
    python json2fasttext.py <INPUT.json> <OUTPUT.txt>

### [Binary Data Set Splitter](./data_acquisition/create_binary_datasets.py)
    python create_binary_datasets.py <INPUT.json> <OUTPUT.txt>


### [Multilabel ARFF Conversion](./data_acquisition/arffConverter_Multilabel.py)

Creates an arff data set, in which exactly one label has to be classified
(either bug, enhancement or question). Use the commands below in the directory of the converters.

```
python arffConverter_Multilabel.py <INPUT.txt> <OUTPUT.arff>
```
> A `StringToWordVector filter` (found in the preprocessing section of the WEKA or MEKA GUI) must be applied to the 
output files, because this preprocessing step is not done automatically.


### [Binary Relevance ARFF Conversion](./data_acquisition/arffConverter_BinaryRelevance.py)
The label we used in WEKA is split up, such 
that each possiblity is its own label (0 or 1).

```
python arffConverter_BinaryRelevance.py <INPUT.txt> <OUTPUT.arff>
```
> A `StringToWordVector filter` (found in the preprocessing section of the WEKA or MEKA GUI) must be applied to the 
output files, because this preprocessing step is not done automatically.

## [Stemming](./stemming)

What is this? 

### [Porter Stemming](./stemming/porter_stemming.py)

 >Requirements
        
    EXAMPLECOMMAND

### [Snowball Stemming](./stemming/snowball_stemming.py)

 >Requirements
        
    EXAMPLECOMMAND

### [Stopword Stemming](./stemming/stopword_stemming.py)

 >Requirements
        
    EXAMPLECOMMAND